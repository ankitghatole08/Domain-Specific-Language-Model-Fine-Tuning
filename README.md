# Domain-Specific LLM Fine-Tuning with Amazon SageMaker

This project demonstrates how to fine-tune a large language model (LLM), specifically **Meta LLaMA 2 7B**, on a domain-specific dataset using **Amazon SageMaker**. The goal is to train the model to generate informative, accurate, and contextually relevant responses within a chosen domain (Finance, Medical, or IT).

---

## ðŸ“Œ Project Objective

Build a **proof-of-concept (PoC)** domain expert model that can:
- Understand and generate domain-specific responses.
- Serve as the backend for internal knowledge apps, chat assistants, or automated text generation.

---

## ðŸš€ Tools & Services Used

- **Amazon SageMaker JumpStart**: For deploying and fine-tuning LLaMA 2 7B.
- **Amazon S3**: Storing input data and model artifacts.
- **Jupyter Notebook on SageMaker Studio**: For orchestration and experimentation.
- **Hugging Face Transformers**: Prompt formatting and testing.

---

## ðŸ“ Project Structure






---

## ðŸ§  Key Steps

### 1. Model Deployment
- Deploy **Meta LLaMA 2 7B** using SageMaker JumpStart via `jumpstart-foundation-models`.

### 2. Base Model Testing
- Evaluate default model performance using handcrafted prompts.
- Analyze general vs. domain-specific response quality.

### 3. Fine-Tuning
- Upload domain-specific dataset to S3.
- Fine-tune LLaMA 2 7B with SageMaker training job.
- Use prompt-response format (Hugging Face compatible) to train.

### 4. Deploy Fine-Tuned Model
- Deploy trained model endpoint from the SageMaker model artifact.
- Test using same prompts as base model.

### 5. Evaluation
- Compare output before and after fine-tuning.
- Measure response quality, relevance, and fluency.

---

## âœ… Results Snapshot

| Prompt | Base Model Response | Fine-Tuned Model Response |
|--------|---------------------|---------------------------|
| â€œExplain cloud load balancing.â€ | Generic answer | More technical, IT-focused answer |
| â€œWhat is API Gateway?â€         | High-level response | Tailored to domain language |

_Screenshots of outputs, SageMaker dashboard, and endpoints are included in the `screenshots/` folder._

---

## ðŸ“˜ What I Learned

- End-to-end LLM fine-tuning on AWS
- Cost management and resource optimization in SageMaker
- Prompt engineering and response evaluation
- Real-world use case implementation for internal domain models

---

## ðŸ Conclusion

This project proves that foundation models like **LLaMA 2** can be fine-tuned cost-effectively to serve specialized needs. The result is a scalable, reusable domain expert that can drive smart applications within an organization.

---

## ðŸ§  Future Work

- Automate deployment pipeline using SageMaker Pipelines
- Add evaluation metrics (BLEU, ROUGE) to scoring
- Deploy front-end chat app using Streamlit or Gradio

---

## ðŸ“· Screenshots

> See the `screenshots/` folder for:
- SageMaker Studio
- Model deployment
- Notebook outputs
- Prompt vs response comparison

---



